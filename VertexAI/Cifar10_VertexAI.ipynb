{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abouslima/AI-Makerspace/blob/master/VertexAI/Cifar10_VertexAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom training and prediction for Vertix AI, using [cifar10 dataset](https://www.tensorflow.org/datasets/catalog/cifar10), from tensorflow.\n",
        "\n",
        "The code is a short replication of this [notebook](https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/custom/sdk-custom-image-classification-online.ipynb#scrollTo=title)\n"
      ],
      "metadata": {
        "id": "MmCkEYGPGjps"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Installing the Vertex SDK and the other dependencies for application using colab notebook**"
      ],
      "metadata": {
        "id": "lguy2v8wHRaY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PijiYrIhRtpZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# The Google Cloud Notebook product has specific requirements\n",
        "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
        "\n",
        "# Google Cloud Notebook requires dependencies to be installed with '--user'\n",
        "USER_FLAG = \"\"\n",
        "if IS_GOOGLE_CLOUD_NOTEBOOK:\n",
        "    USER_FLAG = \"--user\"\n",
        "\n",
        "! pip install {USER_FLAG} --upgrade google-cloud-aiplatform --progress-bar off\n",
        "! pip install {USER_FLAG} --upgrade google-cloud-storage --progress-bar off\n",
        "! pip install {USER_FLAG} --upgrade pillow --progress-bar off\n",
        "! pip install {USER_FLAG} --upgrade numpy --progress-bar off"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**restarting the kernel to load the installed packages**"
      ],
      "metadata": {
        "id": "RhJPcuggH7Pe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yAWtx6XxTOP3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "if not os.getenv(\"IS_TESTING\"):\n",
        "    # Automatically restart kernel after installs\n",
        "    import IPython\n",
        "\n",
        "    app = IPython.Application.instance()\n",
        "    app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Vertex AI should be enabled within a project.**"
      ],
      "metadata": {
        "id": "4e22dB0yIUuf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Saving the timestamp to use in naming the pipeline\n",
        "*   Saving the project name, the bucket for storage and the region in variables\n",
        "\n"
      ],
      "metadata": {
        "id": "R9IYzq2YIrs3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "import os\n",
        "\n",
        "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
        "PROJECT_ID = \"vertex-ai-makerspace\"  # @param {type:\"string\"}\n",
        "BUCKET_NAME = \"gs://my-ai-makerspace-bucket-1212112\"  # @param {type:\"string\"}\n",
        "REGION = \"europe-west4\"  # @param {type:\"string\"}"
      ],
      "metadata": {
        "id": "ImZUoa1yRFc3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Authenticating Google Cloud account**"
      ],
      "metadata": {
        "id": "OCi7xzOxJVJL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DwQTHc5CUd19"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "# If you are running this notebook in Colab, run this cell and follow the\n",
        "# instructions to authenticate your GCP account. This provides access to your\n",
        "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
        "# requests.\n",
        "\n",
        "# The Google Cloud Notebook product has specific requirements\n",
        "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
        "\n",
        "# If on Google Cloud Notebooks, then don't execute this code\n",
        "if not IS_GOOGLE_CLOUD_NOTEBOOK:\n",
        "    if \"google.colab\" in sys.modules:\n",
        "        from google.colab import auth as google_auth\n",
        "\n",
        "        google_auth.authenticate_user()\n",
        "\n",
        "    # If you are running this notebook locally, replace the string below with the\n",
        "    # path to your service account key and run this cell to authenticate your GCP\n",
        "    # account.\n",
        "    elif not os.getenv(\"IS_TESTING\"):\n",
        "        %env GOOGLE_APPLICATION_CREDENTIALS ''"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the following if the storage bucket isn't created in advance"
      ],
      "metadata": {
        "id": "DuMOS40MJuHJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TKjkmdibbM3G"
      },
      "outputs": [],
      "source": [
        "! gsutil mb -p $PROJECT_ID -l $REGION $BUCKET_NAME"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Validating access to the cloud storage bucket"
      ],
      "metadata": {
        "id": "Rk39OzVBJ0Ju"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ZI3Fv1albM8T"
      },
      "outputs": [],
      "source": [
        "! gsutil ls -al $BUCKET_NAME"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importing the AI Platform library and instantiating it**"
      ],
      "metadata": {
        "id": "n4FVpKpaJ9Rn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "s1taEh98BUOi"
      },
      "outputs": [],
      "source": [
        "from google.cloud import aiplatform\n",
        "from google.cloud.aiplatform import gapic as aip\n",
        "\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Specifiying the GPU type in case needed for training or predictions\n",
        "\n",
        "[GPU compatibility with CPU](https://cloud.google.com/ai-platform/training/docs/using-gpus) | \n",
        "[GPU compatibility wiht locations](https://cloud.google.com/vertex-ai/docs/general/locations#europe_2)"
      ],
      "metadata": {
        "id": "aR0CIksFKJwK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ei9R9GpDBhJ1"
      },
      "outputs": [],
      "source": [
        "#aip.AcceleratorType.NVIDIA_TESLA_K80\n",
        "\n",
        "TRAIN_GPU, TRAIN_NGPU = (None, None) # @param {type:\"string\"} \n",
        "\n",
        "DEPLOY_GPU, DEPLOY_NGPU = (None, None) # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Specifiying the container for training and predictions\n",
        "\n",
        "[Pre-Built Training Containers](https://cloud.google.com/vertex-ai/docs/training/pre-built-containers) | [Pre-Built Predicting/Deployment Containiers](https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers)"
      ],
      "metadata": {
        "id": "2yJu_yEPK8wR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4MtauKDCRbp",
        "outputId": "37b3aa45-930f-4ccd-bb01-da374aa9c013"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training: gcr.io/cloud-aiplatform/training/tf-cpu.2-1:latest None None\n",
            "Deployment: gcr.io/cloud-aiplatform/prediction/tf2-cpu.2-1:latest None None\n"
          ]
        }
      ],
      "source": [
        "TRAIN_VERSION = \"tf-cpu.2-1\" # @param {type:\"string\"}\n",
        "DEPLOY_VERSION = \"tf2-cpu.2-1\" # @param {type:\"string\"}\n",
        "\n",
        "TRAIN_IMAGE = \"gcr.io/cloud-aiplatform/training/{}:latest\".format(TRAIN_VERSION)\n",
        "DEPLOY_IMAGE = \"gcr.io/cloud-aiplatform/prediction/{}:latest\".format(DEPLOY_VERSION)\n",
        "\n",
        "print(\"Training:\", TRAIN_IMAGE, TRAIN_GPU, TRAIN_NGPU)\n",
        "print(\"Deployment:\", DEPLOY_IMAGE, DEPLOY_GPU, DEPLOY_NGPU)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Specifiying training and Deployment Machine type and number of vCPU's"
      ],
      "metadata": {
        "id": "iddMdABnLg7H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kurDlAT6CRgd",
        "outputId": "89a66124-78b7-477f-c411-f103f8f531d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train machine type n1-standard-8\n",
            "Deploy machine type n1-standard-8\n"
          ]
        }
      ],
      "source": [
        "MACHINE_TYPE = \"n1-standard\" # @param {type:\"string\"}\n",
        "\n",
        "VCPU = \"8\" # @param {type:\"string\"}\n",
        "TRAIN_COMPUTE = MACHINE_TYPE + \"-\" + VCPU\n",
        "print(\"Train machine type\", TRAIN_COMPUTE)\n",
        "\n",
        "MACHINE_TYPE = \"n1-standard\" # @param {type:\"string\"}\n",
        "\n",
        "VCPU = \"8\" # @param {type:\"string\"}\n",
        "DEPLOY_COMPUTE = MACHINE_TYPE + \"-\" + VCPU\n",
        "print(\"Deploy machine type\", DEPLOY_COMPUTE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "hDNAqHkiCRi3"
      },
      "outputs": [],
      "source": [
        "JOB_NAME = \"custom_job_\" + TIMESTAMP  # the name of the experiment\n",
        "\n",
        "\n",
        "if not TRAIN_NGPU or TRAIN_NGPU < 2:\n",
        "    TRAIN_STRATEGY = \"single\"\n",
        "else:\n",
        "    TRAIN_STRATEGY = \"mirror\"\n",
        "\n",
        "EPOCHS = 20 # @param {type:\"integer\"}\n",
        "STEPS = 100 # @param {type:\"integer\"}\n",
        "\n",
        "# Defining the command arguments for the training script\n",
        "CMDARGS = [\n",
        "    \"--epochs=\" + str(EPOCHS),\n",
        "    \"--steps=\" + str(STEPS),\n",
        "    \"--distribute=\" + TRAIN_STRATEGY,\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following script loads, builds, compiles, trains and save the model to the specified directory \"cloud storage\"\n",
        "\n",
        "[Model Export documentation](https://cloud.google.com/vertex-ai/docs/training/exporting-model-artifacts#tensorflow)"
      ],
      "metadata": {
        "id": "Ej7SN9hWN5xF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5qUJqgmdCRlx"
      },
      "outputs": [],
      "source": [
        "%%writefile task.py\n",
        "# Single, Mirror and Multi-Machine Distributed Training for CIFAR-10\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.client import device_lib\n",
        "import argparse\n",
        "import os\n",
        "import sys\n",
        "tfds.disable_progress_bar()\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--lr', dest='lr',\n",
        "                    default=0.01, type=float,\n",
        "                    help='Learning rate.')\n",
        "parser.add_argument('--epochs', dest='epochs',\n",
        "                    default=10, type=int,\n",
        "                    help='Number of epochs.')\n",
        "parser.add_argument('--steps', dest='steps',\n",
        "                    default=200, type=int,\n",
        "                    help='Number of steps per epoch.')\n",
        "parser.add_argument('--distribute', dest='distribute', type=str, default='single',\n",
        "                    help='distributed training strategy')\n",
        "args = parser.parse_args()\n",
        "\n",
        "print('Python Version = {}'.format(sys.version))\n",
        "print('TensorFlow Version = {}'.format(tf.__version__))\n",
        "print('TF_CONFIG = {}'.format(os.environ.get('TF_CONFIG', 'Not found')))\n",
        "print('DEVICES', device_lib.list_local_devices())\n",
        "\n",
        "# Single Machine, single compute device\n",
        "if args.distribute == 'single':\n",
        "    if tf.test.is_gpu_available():\n",
        "        strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")\n",
        "    else:\n",
        "        strategy = tf.distribute.OneDeviceStrategy(device=\"/cpu:0\")\n",
        "# Single Machine, multiple compute device\n",
        "elif args.distribute == 'mirror':\n",
        "    strategy = tf.distribute.MirroredStrategy()\n",
        "# Multiple Machine, multiple compute device\n",
        "elif args.distribute == 'multi':\n",
        "    strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\n",
        "\n",
        "# Multi-worker configuration\n",
        "print('num_replicas_in_sync = {}'.format(strategy.num_replicas_in_sync))\n",
        "\n",
        "# Preparing dataset\n",
        "BUFFER_SIZE = 10000\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "def make_datasets_unbatched():\n",
        "  # Scaling CIFAR10 data from (0, 255] to (0., 1.]\n",
        "  def scale(image, label):\n",
        "    image = tf.cast(image, tf.float32)\n",
        "    image /= 255.0\n",
        "    return image, label\n",
        "\n",
        "  datasets, info = tfds.load(name='cifar10',\n",
        "                            with_info=True,\n",
        "                            as_supervised=True)\n",
        "  return datasets['train'].map(scale).cache().shuffle(BUFFER_SIZE).repeat()\n",
        "\n",
        "\n",
        "# Build the Keras model\n",
        "def build_and_compile_cnn_model():\n",
        "  model = tf.keras.Sequential([\n",
        "      tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(32, 32, 3)),\n",
        "      tf.keras.layers.MaxPooling2D(),\n",
        "      tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
        "      tf.keras.layers.MaxPooling2D(),\n",
        "      tf.keras.layers.Flatten(),\n",
        "      tf.keras.layers.Dense(10, activation='softmax')\n",
        "  ])\n",
        "  model.compile(\n",
        "      loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
        "      optimizer=tf.keras.optimizers.SGD(learning_rate=args.lr),\n",
        "      metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "# Train the model\n",
        "NUM_WORKERS = strategy.num_replicas_in_sync\n",
        "# Here the batch size scales up by number of workers since\n",
        "# `tf.data.Dataset.batch` expects the global batch size.\n",
        "GLOBAL_BATCH_SIZE = BATCH_SIZE * NUM_WORKERS\n",
        "MODEL_DIR = os.getenv(\"AIP_MODEL_DIR\")\n",
        "\n",
        "train_dataset = make_datasets_unbatched().batch(GLOBAL_BATCH_SIZE)\n",
        "\n",
        "with strategy.scope():\n",
        "  # Creation of dataset, and model building/compiling need to be within\n",
        "  # `strategy.scope()`.\n",
        "  model = build_and_compile_cnn_model()\n",
        "\n",
        "model.fit(x=train_dataset, epochs=args.epochs, steps_per_epoch=args.steps)\n",
        "model.save(MODEL_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining the custom training job."
      ],
      "metadata": {
        "id": "VLxQvTAOOVuP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Xsuof0RCRoJ"
      },
      "outputs": [],
      "source": [
        "job = aiplatform.CustomTrainingJob(\n",
        "    display_name=JOB_NAME, \n",
        "    script_path=\"task.py\",\n",
        "    container_uri=TRAIN_IMAGE, #the container image for training\n",
        "    requirements=[\"tensorflow_datasets==1.3.0\"], # any additional requirements\n",
        "    model_serving_container_image_uri=DEPLOY_IMAGE, #the deployment container\n",
        ")\n",
        "\n",
        "MODEL_DISPLAY_NAME = \"cifar10-\" + TIMESTAMP\n",
        "\n",
        "# Start the training\n",
        "if TRAIN_GPU:\n",
        "    model = job.run(\n",
        "        model_display_name=MODEL_DISPLAY_NAME,\n",
        "        args=CMDARGS,\n",
        "        replica_count=1,\n",
        "        machine_type=TRAIN_COMPUTE,\n",
        "        accelerator_type=TRAIN_GPU.name,\n",
        "        accelerator_count=TRAIN_NGPU,\n",
        "    )\n",
        "else:\n",
        "    model = job.run(\n",
        "        model_display_name=MODEL_DISPLAY_NAME,\n",
        "        args=CMDARGS,\n",
        "        replica_count=1,\n",
        "        machine_type=TRAIN_COMPUTE,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Deploying the trained model**"
      ],
      "metadata": {
        "id": "w0mLMlUEPOi7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xayowRyYK58U"
      },
      "outputs": [],
      "source": [
        "DEPLOYED_NAME = \"cifar10_deployed-\" + TIMESTAMP\n",
        "\n",
        "TRAFFIC_SPLIT = {\"0\": 100}\n",
        "\n",
        "MIN_NODES = 1\n",
        "MAX_NODES = 1\n",
        "\n",
        "if DEPLOY_GPU:\n",
        "    endpoint = model.deploy(\n",
        "        deployed_model_display_name=DEPLOYED_NAME,\n",
        "        traffic_split=TRAFFIC_SPLIT,\n",
        "        machine_type=DEPLOY_COMPUTE,\n",
        "        accelerator_type=DEPLOY_GPU.name,\n",
        "        accelerator_count=DEPLOY_NGPU,\n",
        "        min_replica_count=MIN_NODES,\n",
        "        max_replica_count=MAX_NODES,\n",
        "    )\n",
        "else:\n",
        "    endpoint = model.deploy(\n",
        "        deployed_model_display_name=DEPLOYED_NAME,\n",
        "        traffic_split=TRAFFIC_SPLIT,\n",
        "        machine_type=DEPLOY_COMPUTE,\n",
        "        min_replica_count=MIN_NODES,\n",
        "        max_replica_count=MAX_NODES\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading and preprocessing the test dataset"
      ],
      "metadata": {
        "id": "yssH_aiRO1ik"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m4oSvdLyG-vT"
      },
      "outputs": [],
      "source": [
        "# Download the images\n",
        "! gsutil -m cp -r gs://cloud-samples-data/ai-platform-unified/cifar_test_images ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ocnV03s68gmD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "# Load image data\n",
        "IMAGE_DIRECTORY = \"cifar_test_images\"\n",
        "\n",
        "image_files = [file for file in os.listdir(IMAGE_DIRECTORY) if file.endswith(\".jpg\")]\n",
        "\n",
        "# Decode JPEG images into numpy arrays\n",
        "image_data = [\n",
        "    np.asarray(Image.open(os.path.join(IMAGE_DIRECTORY, file))) for file in image_files\n",
        "]\n",
        "\n",
        "# Scale and convert to expected format\n",
        "x_test = [(image / 255.0).astype(np.float32).tolist() for image in image_data]\n",
        "\n",
        "# Extract labels from image name\n",
        "y_test = [int(file.split(\"_\")[1]) for file in image_files]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finding the accuracy of the predictions"
      ],
      "metadata": {
        "id": "MWY9WTB_QU-c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = endpoint.predict(instances=x_test)\n",
        "y_predicted = np.argmax(predictions.predictions, axis=1)\n",
        "\n",
        "correct = sum(y_predicted == np.array(y_test))\n",
        "accuracy = len(y_predicted)\n",
        "print(\n",
        "    f\"Correct predictions = {correct}, Total predictions = {accuracy}, Accuracy = {correct/accuracy}\"\n",
        ")"
      ],
      "metadata": {
        "id": "F5aip-awQTgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If deployed using the UI"
      ],
      "metadata": {
        "id": "QnC2ywFFQHiB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Xnbb4XrVQJDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Alt5E4H_8iUz"
      },
      "outputs": [],
      "source": [
        "ENDPOINT_ID=\"4759794632735326208\" # @param {type:\"string\"}\n",
        "PROJECT_ID=\"249560904503\" # @param {type:\"string\"}\n",
        "\n",
        "endpoint_name= f\"projects/{PROJECT_ID}/locations/europe-west4/endpoints/{ENDPOINT_ID}\"\n",
        "endpoint = aiplatform.Endpoint(endpoint_name=endpoint_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MMNk64is89m7"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ulA6yC2Y9OXQ"
      },
      "outputs": [],
      "source": [
        "predictions = endpoint.predict(instances=x_test)\n",
        "y_predicted = np.argmax(predictions.predictions, axis=1)\n",
        "\n",
        "correct = sum(y_predicted == np.array(y_test))\n",
        "accuracy = len(y_predicted)\n",
        "print(\n",
        "    f\"Correct predictions = {correct}, Total predictions = {accuracy}, Accuracy = {correct/accuracy}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zqsXfeZj9P3M"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Undeploy the model"
      ],
      "metadata": {
        "id": "mosB00zig4Vs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pa5ItIaw-pzE"
      },
      "outputs": [],
      "source": [
        "deployed_model_id = endpoint.list_models()[0].id\n",
        "endpoint.undeploy(deployed_model_id=deployed_model_id)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Cifar10_VertexAI.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMnXUlc5m3YCq7ER7aoaMmp",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}