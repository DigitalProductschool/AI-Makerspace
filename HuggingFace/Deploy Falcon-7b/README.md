# Deploying OpenSource LLMs on SageMaker

This repository contains the code for deploying **[Falcon-7B-Instruct ðŸ¦…](https://huggingface.co/tiiuae/falcon-7b-instruct)** which is a FOSS model.

**ðŸŽ¥ Please click the link to**
[watch the workshop recording](https://drive.google.com/file/d/1yrZkI0accmyItSk8hOIqP_txDTPgKZzZ/view?usp=sharing).

## About Falcon-7B-Instruct

`Falcon-7B-Instruct` is a 7B parameters causal decoder-only model built by TII based on Falcon-7B and finetuned on a mixture of chat/instruct datasets. It is made available under the Apache 2.0 license.

## Deployment Architecture

![Deployment Architecture](Deployment%20Architecture.png)
_Image source: towardsai.net_

## Deployment Steps

1. Deploy the model on AWS SageMaker. (See the code in `Falcon-7b-deploy.ipynb`)

2. Create a Lambda function to invoke the SageMaker endpoint. (See the code in `lambda.py`)
3. Create an API Gateway to invoke the Lambda function. This HTTP/REST API will act as the trigger for the Lambda function.

## Deployment Details

- This is the permission policy that needs to be attached to the Lambda function, this allows the Lambda function to invoke the SageMaker endpoint.

```
  {
    "Effect": "Allow",
    "Action": "sagemaker:InvokeEndpoint",
    "Resource": "<ARN>"
  }
```

- Also don't forget to add the `ENDPOINT_NAME` in the Lambda function environment variables.

## Sample Payload

This payload will be passed in the body of the API request.

```json
{
  "inputs": "Where is Munich? and is it beautiful?",
  "parameters": {
    "top_p": 0.9,
    "temperature": 0.8,
    "max_new_tokens": 512,
    "repetition_penalty": 1.03
  }
}
```

## Model Inference Parameters

These parameters are crucial in shaping the output generated by the model.

**`top_p`**

- **Description:** `top_p` is a parameter that controls the nucleus sampling probability. It determines the cumulative probability threshold for the model's output. Tokens with probabilities exceeding this threshold are considered, and the sampling continues until the cumulative probability exceeds `top_p`.
- **Value in `params` dictionary:** 0.9

**`temperature`**

- **Description:** `temperature` is a parameter that scales the logits before sampling. A higher temperature value (e.g., 1.0) makes the sampling more random, while a lower value (e.g., 0.1) makes it more deterministic.
- **Value in `params` dictionary:** 0.8

**`max_new_tokens`**

- **Description:** `max_new_tokens` specifies the maximum number of tokens that the model should generate as output. It can be used to limit the length of the generated text.
- **Value in `params` dictionary:** 512

**`repetition_penalty`**

- **Description:** `repetition_penalty` is a parameter that discourages the model from repeating the same tokens in the generated output. A higher value penalizes repetitions more, making the output less repetitive.
- **Value in `params` dictionary:** 1.03

## Clean Up ðŸ§¹âœ¨

1. Stop the notebook instance.
2. From the inference tab of the SageMaker console, delete the model, endpoint, endpoint configuration.

Or follow this official [AWS Guide](https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-cleanup.html) for cleanup.

## References

- [Deploy Falcon 7B & 40B on Amazon SageMaker
  ](https://www.philschmid.de/sagemaker-falcon-llm)
- [Deploying Open-Source LLMs As APIs
  ](https://skanda-vivek.medium.com/deploying-open-source-llms-as-apis-ec026e2187bc)
