{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abouslima/AI-Makerspace/blob/master/VertexAI/PyCaret_VertexAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom training and prediction for Vertix AI, using pycaret model"
      ],
      "metadata": {
        "id": "MmCkEYGPGjps"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Installing the Vertex SDK and the other dependencies for application using colab notebook**"
      ],
      "metadata": {
        "id": "lguy2v8wHRaY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PijiYrIhRtpZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# The Google Cloud Notebook product has specific requirements\n",
        "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
        "\n",
        "# Google Cloud Notebook requires dependencies to be installed with '--user'\n",
        "USER_FLAG = \"\"\n",
        "if IS_GOOGLE_CLOUD_NOTEBOOK:\n",
        "    USER_FLAG = \"--user\"\n",
        "\n",
        "! pip install {USER_FLAG} --upgrade google-cloud-aiplatform\n",
        "! pip install {USER_FLAG} --upgrade google-cloud-storage\n",
        "! pip install {USER_FLAG} --upgrade pillow\n",
        "! pip install {USER_FLAG} --upgrade numpy\n",
        "\n",
        "if not os.getenv(\"IS_TESTING\"):\n",
        "    # Automatically restart kernel after installs\n",
        "    import IPython\n",
        "\n",
        "    app = IPython.Application.instance()\n",
        "    app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8kZ7PE_FJpk"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Vertex AI should be enabled within a project.**"
      ],
      "metadata": {
        "id": "4e22dB0yIUuf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Saving the timestamp to use in naming the pipeline\n",
        "*   Saving the project name, the bucket for storage and the region in variables\n",
        "\n"
      ],
      "metadata": {
        "id": "R9IYzq2YIrs3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "import os\n",
        "\n",
        "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
        "PROJECT_ID = \"vertex-ai-makerspace\"  # @param {type:\"string\"}\n",
        "BUCKET_NAME = \"gs://my-ai-makerspace-bucket-pycaret\"  # @param {type:\"string\"}\n",
        "REGION = \"europe-west4\"  # @param {type:\"string\"}"
      ],
      "metadata": {
        "id": "ImZUoa1yRFc3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Authenticating Google Cloud account**"
      ],
      "metadata": {
        "id": "OCi7xzOxJVJL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DwQTHc5CUd19"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "# If you are running this notebook in Colab, run this cell and follow the\n",
        "# instructions to authenticate your GCP account. This provides access to your\n",
        "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
        "# requests.\n",
        "\n",
        "# The Google Cloud Notebook product has specific requirements\n",
        "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
        "\n",
        "# If on Google Cloud Notebooks, then don't execute this code\n",
        "if not IS_GOOGLE_CLOUD_NOTEBOOK:\n",
        "    if \"google.colab\" in sys.modules:\n",
        "        from google.colab import auth as google_auth\n",
        "\n",
        "        google_auth.authenticate_user()\n",
        "\n",
        "    # If you are running this notebook locally, replace the string below with the\n",
        "    # path to your service account key and run this cell to authenticate your GCP\n",
        "    # account.\n",
        "    elif not os.getenv(\"IS_TESTING\"):\n",
        "        %env GOOGLE_APPLICATION_CREDENTIALS ''"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the following if the storage bucket isn't created in advance"
      ],
      "metadata": {
        "id": "DuMOS40MJuHJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKjkmdibbM3G",
        "outputId": "dbdb734f-f969-4185-f9e1-8402ae3bff34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating gs://my-ai-makerspace-bucket-1212112/...\n"
          ]
        }
      ],
      "source": [
        "! gsutil mb -p $PROJECT_ID -l $REGION $BUCKET_NAME"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Validating access to the cloud storage bucket"
      ],
      "metadata": {
        "id": "Rk39OzVBJ0Ju"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZI3Fv1albM8T"
      },
      "outputs": [],
      "source": [
        "! gsutil ls -al $BUCKET_NAME"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importing the AI Platform library and instantiating it**"
      ],
      "metadata": {
        "id": "n4FVpKpaJ9Rn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s1taEh98BUOi"
      },
      "outputs": [],
      "source": [
        "from google.cloud import aiplatform\n",
        "#from google.cloud.aiplatform import gapic as aip\n",
        "\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Specifiying the container for training and predictions\n",
        "\n",
        "[Pre-Built Training Containers](https://cloud.google.com/vertex-ai/docs/training/pre-built-containers) | [Pre-Built Predicting/Deployment Containiers](https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers)"
      ],
      "metadata": {
        "id": "2yJu_yEPK8wR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4MtauKDCRbp"
      },
      "outputs": [],
      "source": [
        "TRAIN_VERSION = \"scikit-learn-cpu.0-23\" # @param {type:\"string\"}\n",
        "DEPLOY_VERSION = \"sklearn-cpu.0-23\" # @param {type:\"string\"}\n",
        "\n",
        "TRAIN_IMAGE = \"gcr.io/cloud-aiplatform/training/{}:latest\".format(TRAIN_VERSION)\n",
        "DEPLOY_IMAGE = \"gcr.io/cloud-aiplatform/prediction/{}:latest\".format(DEPLOY_VERSION)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Specifiying training and Deployment Machine type and number of vCPU's"
      ],
      "metadata": {
        "id": "iddMdABnLg7H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kurDlAT6CRgd"
      },
      "outputs": [],
      "source": [
        "MACHINE_TYPE = \"n1-standard\" # @param {type:\"string\"}\n",
        "\n",
        "VCPU = \"8\" # @param {type:\"string\"}\n",
        "TRAIN_COMPUTE = MACHINE_TYPE + \"-\" + VCPU\n",
        "print(\"Train machine type\", TRAIN_COMPUTE)\n",
        "\n",
        "MACHINE_TYPE = \"n1-standard\" # @param {type:\"string\"}\n",
        "\n",
        "VCPU = \"8\" # @param {type:\"string\"}\n",
        "DEPLOY_COMPUTE = MACHINE_TYPE + \"-\" + VCPU\n",
        "print(\"Deploy machine type\", DEPLOY_COMPUTE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hDNAqHkiCRi3"
      },
      "outputs": [],
      "source": [
        "JOB_NAME = \"custom_job_pycaret_\" + TIMESTAMP  # the name of the experiment"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following script loads, builds, compiles, trains and save the model to the specified directory \"cloud storage\"\n",
        "\n",
        "[Model Export documentation](https://cloud.google.com/vertex-ai/docs/training/exporting-model-artifacts#tensorflow)"
      ],
      "metadata": {
        "id": "Ej7SN9hWN5xF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5qUJqgmdCRlx"
      },
      "outputs": [],
      "source": [
        "%%writefile task.py\n",
        "\n",
        "from google.cloud import storage\n",
        "import pandas as pd\n",
        "from pycaret.classification import *\n",
        "import os\n",
        "import joblib\n",
        "\n",
        "dataset = pd.read_csv(\"https://raw.githubusercontent.com/DigitalProductschool/AI-Makerspace/master/PyCaret-Classification/UniversalBank.csv\")\n",
        "dataset.columns = [i.replace(\" \", \"\") for i in dataset.columns]\n",
        "dataset.drop([\"ID\",\"ZIPCode\"],axis=1,inplace=True)\n",
        "cat_cols = [\"Family\",\"Education\",\"SecuritiesAccount\",\"CDAccount\",\"Online\",\"CreditCard\"]\n",
        "data = dataset.sample(frac=0.9, random_state=786)\n",
        "data_unseen = dataset.drop(data.index)\n",
        "\n",
        "data.reset_index(drop=True, inplace=True)\n",
        "data_unseen.reset_index(drop=True, inplace=True)\n",
        "exp_1 = setup(data = data, \n",
        "                    session_id=123, \n",
        "                    target = 'PersonalLoan', \n",
        "                    categorical_features=cat_cols,\n",
        "                    normalize=True, \n",
        "                    normalize_method='minmax',\n",
        "                    transformation=True,\n",
        "                    use_gpu=False,\n",
        "                    log_experiment=True,\n",
        "                    experiment_name='loan1',\n",
        "                    silent=True)\n",
        "\n",
        "best = compare_models()\n",
        "\n",
        "tuned_model = tune_model(best, optimize = 'AUC') # Optimize - Measure used to select the best model through hyperparameter tuning.\n",
        "final_model = finalize_model(tuned_model, model_only=True)\n",
        "\n",
        "\n",
        "artifact_filename = 'model.joblib'\n",
        "\n",
        "# Save model artifact to local filesystem (doesn't persist)\n",
        "local_path = artifact_filename\n",
        "joblib.dump(final_model, local_path)\n",
        "\n",
        "# Upload model artifact to Cloud Storage\n",
        "model_directory = os.environ['AIP_MODEL_DIR']\n",
        "storage_path = os.path.join(model_directory, artifact_filename)\n",
        "blob = storage.blob.Blob.from_string(storage_path, client=storage.Client())\n",
        "blob.upload_from_filename(local_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining the custom training job."
      ],
      "metadata": {
        "id": "VLxQvTAOOVuP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Xsuof0RCRoJ"
      },
      "outputs": [],
      "source": [
        "job = aiplatform.CustomTrainingJob(\n",
        "    display_name=JOB_NAME,\n",
        "    script_path=\"task.py\",\n",
        "    container_uri=TRAIN_IMAGE,\n",
        "    requirements=[\"pycaret\", \"numpy==1.19.5\"],\n",
        "    model_serving_container_image_uri=DEPLOY_IMAGE\n",
        ")\n",
        "\n",
        "MODEL_DISPLAY_NAME = \"pycaret-\" + TIMESTAMP\n",
        "\n",
        "model = job.run(\n",
        "    model_display_name=MODEL_DISPLAY_NAME,\n",
        "    replica_count=1,\n",
        "    machine_type=TRAIN_COMPUTE,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Deploying the trained model**"
      ],
      "metadata": {
        "id": "w0mLMlUEPOi7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xayowRyYK58U"
      },
      "outputs": [],
      "source": [
        "DEPLOYED_NAME = \"pycaret-\" + TIMESTAMP\n",
        "\n",
        "TRAFFIC_SPLIT = {\"0\": 100}\n",
        "\n",
        "MIN_NODES = 1\n",
        "MAX_NODES = 1\n",
        "\n",
        "\n",
        "endpoint = model.deploy(\n",
        "    deployed_model_display_name=DEPLOYED_NAME,\n",
        "    traffic_split=TRAFFIC_SPLIT,\n",
        "    machine_type=DEPLOY_COMPUTE,\n",
        "    min_replica_count=MIN_NODES,\n",
        "    max_replica_count=MAX_NODES\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading and preprocessing the test dataset"
      ],
      "metadata": {
        "id": "yssH_aiRO1ik"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m4oSvdLyG-vT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "dataset = pd.read_csv(\"https://raw.githubusercontent.com/DigitalProductschool/AI-Makerspace/master/PyCaret-Classification/UniversalBank.csv\")\n",
        "dataset.columns = [i.replace(\" \", \"\") for i in dataset.columns]\n",
        "dataset.drop([\"ID\",\"ZIPCode\"],axis=1,inplace=True)\n",
        "cat_cols = [\"Family\",\"Education\"]\n",
        "data = dataset.sample(frac=0.9, random_state=786)\n",
        "data_unseen = dataset.drop(data.index)\n",
        "data.reset_index(drop=True, inplace=True)\n",
        "data_unseen.reset_index(drop=True, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ocnV03s68gmD"
      },
      "outputs": [],
      "source": [
        "X_train = pd.get_dummies(data, columns=cat_cols).drop([\"PersonalLoan\"], axis=1)\n",
        "X_test = pd.get_dummies(data_unseen, columns=cat_cols).drop([\"PersonalLoan\"], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finding the accuracy of the predictions"
      ],
      "metadata": {
        "id": "MWY9WTB_QU-c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)\n",
        "predictions = endpoint.predict(instances=X_test.tolist())\n",
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(data_unseen.PersonalLoan.values, predictions[0])"
      ],
      "metadata": {
        "id": "F5aip-awQTgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If deployed using the UI"
      ],
      "metadata": {
        "id": "QnC2ywFFQHiB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Xnbb4XrVQJDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Alt5E4H_8iUz"
      },
      "outputs": [],
      "source": [
        "ENDPOINT_ID=\"4759794632735326208\" # @param {type:\"string\"}\n",
        "PROJECT_ID=\"249560904503\" # @param {type:\"string\"}\n",
        "\n",
        "endpoint_name= f\"projects/{PROJECT_ID}/locations/europe-west4/endpoints/{ENDPOINT_ID}\"\n",
        "endpoint = aiplatform.Endpoint(endpoint_name=endpoint_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MMNk64is89m7"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ulA6yC2Y9OXQ"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)\n",
        "predictions = endpoint.predict(instances=X_test.tolist())\n",
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(data_unseen.PersonalLoan.values, predictions[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zqsXfeZj9P3M"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Undeploy the model"
      ],
      "metadata": {
        "id": "mosB00zig4Vs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pa5ItIaw-pzE"
      },
      "outputs": [],
      "source": [
        "deployed_model_id = endpoint.list_models()[0].id\n",
        "endpoint.undeploy(deployed_model_id=deployed_model_id)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "PyCaret_VertexAI.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOe00UY2+qacHP2nbPaJFMT",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}