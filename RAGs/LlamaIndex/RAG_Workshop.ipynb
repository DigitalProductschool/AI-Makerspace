{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rWEEsswsz40",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "750800f4-3f63-4bf7-f4fc-e0fc241d6603"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KknZMPi5vY95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13e86ffa-c32d-44a8-81f9-e67f98d4d9ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/RAG_Workshop\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/RAG_Workshop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l683__iMvm6T",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "504005ac-8134-4260-95ac-44b64c9bce74"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/RAG_Workshop'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "%pwd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q llama-index qdrant_client llama-index-vector-stores-qdrant python-dotenv llama-index-experimental"
      ],
      "metadata": {
        "id": "3Ztj2lFbSWmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_print(text, max_line_length=80):\n",
        "    words = text.split(' ')\n",
        "    line = ''\n",
        "    for word in words:\n",
        "        if len(line) + len(word) + 1 <= max_line_length:\n",
        "            line += word + ' '\n",
        "        else:\n",
        "            print(line)\n",
        "            line = word + ' '\n",
        "    print(line)"
      ],
      "metadata": {
        "id": "wHIbWRminR2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cZgLGsLBetu5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "3rU3O4dvpL2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic RAG\n",
        "\n",
        "Retrieval-Augmented Generation (RAG) combines the strengths of retrieval-based and generative AI models to enhance the context and accuracy of AI-generated text. In this section, we introduce the fundamental components of a basic RAG system. We'll start by loading our documents, breaking them down into manageable chunks, and converting these chunks into embeddings, which are numerical representations suitable for machine processing.\n",
        "\n",
        "![Basic RAG Pipeline](https://miro.medium.com/v2/resize:fit:1200/1*J7vyY3EjY46AlduMvr9FbQ.png)\n",
        "*Image Source: [Source](https://medium.com/@drjulija/what-is-retrieval-augmented-generation-rag-938e4f6e03d1)*"
      ],
      "metadata": {
        "id": "e4siIO9d78_u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading document\n",
        "\n",
        "Our first step involves loading the documents that our RAG system will use as its knowledge base. These documents can range from simple text files to complex PDFs containing vast amounts of information. Once loaded, we split each document into smaller segments, known as 'chunks'. This process is crucial for making the data more manageable for the subsequent embedding step, ensuring that our model can efficiently process and retrieve information from these documents.\n"
      ],
      "metadata": {
        "id": "IEJJP3-o87vG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import SimpleDirectoryReader\n",
        "\n",
        "# load data\n",
        "documents = SimpleDirectoryReader(input_files=[\"./transcripts/AIAct.pdf\"]).load_data()"
      ],
      "metadata": {
        "id": "WrkVL9jWSWfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Vector Store\n",
        "\n",
        "After chunking our documents, we need a place to store the resulting embeddings. This is where the Vector Store comes into play. Think of it as a specialized database optimized for storing and querying high-dimensional vectors. By storing our embeddings in the Vector Store, we facilitate efficient retrieval of relevant document chunks based on user queries, laying the groundwork for our RAG system's retrieval component.\n",
        "\n",
        "![Vector Store](https://miro.medium.com/v2/resize:fit:1400/1*wyaikzoEA397xahVKEscnA.png)\n",
        "*Image Source: [Source](https://medium.com/aimonks/introduction-to-chromadb-vector-store-for-generative-ai-llms-28f90535086)*"
      ],
      "metadata": {
        "id": "eJEqX4R09I9M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import StorageContext\n",
        "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
        "import qdrant_client\n",
        "\n",
        "# define the vector_store\n",
        "client = qdrant_client.QdrantClient(location=\":memory:\")\n",
        "vector_store = QdrantVectorStore(client=client, collection_name=\"test_store\")\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
      ],
      "metadata": {
        "id": "DHcw24RgSWcE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the basic index for querying\n",
        "\n",
        "With our Vector Store ready, the next step involves creating an index to organize our embeddings in a way that optimizes retrieval performance. This index acts as a searchable catalog of embeddings, allowing our RAG system to quickly find the most relevant document chunks in response to a query. This process is akin to how a library's indexing system enables fast retrieval of books based on specific topics or keywords.\n"
      ],
      "metadata": {
        "id": "yfsYuExK9Ub7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import Settings, VectorStoreIndex\n",
        "from dotenv import load_dotenv\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "\n",
        "load_dotenv(\"env\")\n",
        "\n",
        "RagLLM=OpenAI(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    temperature=0.2\n",
        "    )\n",
        "embed_model=OpenAIEmbedding()\n",
        "\n",
        "Settings.llm = RagLLM\n",
        "Settings.embed_model = embed_model\n",
        "Settings.chunk_size = 512\n",
        "\n",
        "\n",
        "# build VectorStoreIndex that takes care of chunking documents\n",
        "# and encoding chunks to embeddings for future retrieval\n",
        "index = VectorStoreIndex.from_documents(documents,storage_context=storage_context)"
      ],
      "metadata": {
        "id": "SLcCrOZMSWY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining the query engine\n",
        "\n",
        "The heart of our RAG system is the Query Engine, which orchestrates the retrieval and generation steps. It utilizes the previously set up index to fetch relevant document chunks and then leverages a Large Language Model (LLM) to generate coherent and contextually appropriate responses based on those chunks. This seamless integration of retrieval and generation is what makes RAG systems particularly powerful for tasks requiring nuanced understanding and synthesis of information."
      ],
      "metadata": {
        "id": "RxGbvv7Z9aNe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The QueryEngine class is equipped with the generator\n",
        "# and facilitates the retrieval and generation steps\n",
        "query_engine = index.as_query_engine()"
      ],
      "metadata": {
        "id": "8k8v7OEloP6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What does the document say about emotion recognition?\""
      ],
      "metadata": {
        "id": "emJTr2pMSWVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Semantic Similarity](https://miro.medium.com/v2/resize:fit:1400/1*pAfG2R9dO8_SWvbkRNg4bg.png)\n",
        "*Image Source: [Source](https://medium.com/@adrian.white/cosine-similarity-in-snowflake-ove-eed3b57f4e6f)*"
      ],
      "metadata": {
        "id": "H16LOZRSgbBQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D6e9AtOkSWSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.response.pprint_utils import pprint_response\n",
        "\n",
        "# Use your Default RAG\n",
        "response = query_engine.query(question)\n",
        "pprint_response(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ElaQgF8BVdas",
        "outputId": "84723238-7e85-4643-cf13-6051c1b1ff8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Response: The document mentions that there are concerns about\n",
            "the definitions of 'emotion recognition' being technically flawed and\n",
            "recommends adjustments. Additionally, AccessNow calls for a wider ban\n",
            "on the use of AI for emotion recognition, categorizing people based on\n",
            "physiological, behavioral, or biometric data, as well as dangerous\n",
            "uses in the context of policing, migration, asylum, and border\n",
            "management.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dBmnp0okSWPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Success Requirements for RAG\n",
        "\n",
        "In order for a RAG system to be deemed as a success (in the sense of providing useful and relevant answers to user questions), there are really only two high level requirements:\n",
        "\n",
        "1- Retrieval must be able to find the most relevant documents to a user query.\n",
        "\n",
        "2- Generation must be able to make good use of the retrieved documents to sufficiently answer the user query."
      ],
      "metadata": {
        "id": "73VcykYlzHzr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced Rag\n",
        "\n",
        "As we move beyond basic RAG, we explore advanced techniques to refine and enhance our system's performance. This includes optimizing the size of our document chunks for balanced retrieval and generation, improving the relevancy of retrieved content through semantic reranking, and employing sophisticated response synthesizers for more coherent outputs. These enhancements aim to address common challenges and elevate the quality of the RAG-generated text.\n",
        "\n",
        "![Advanced RAG](https://miro.medium.com/v2/resize:fit:2000/0*Gr_JqzdpHu7enWG9.png)\n",
        "*Image Source: [Source](https://pub.towardsai.net/advanced-rag-techniques-an-illustrated-overview-04d193d8fec6)*\n"
      ],
      "metadata": {
        "id": "W9dfIXGEy8jd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chunk-Size Optimization\n",
        "\n",
        "Chunk-Size Optimization is a critical process in enhancing the performance of a RAG system. It involves tuning the size of the text chunks that are processed by the system to ensure an optimal balance between retrieval efficiency and the quality of generated responses. The right chunk size can significantly impact the system's ability to retrieve relevant information and generate coherent and contextually appropriate responses. This section will explore strategies for determining the optimal chunk size through empirical testing and evaluation.\n"
      ],
      "metadata": {
        "id": "4M1FB16-zfP2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import load_index_from_storage\n",
        "from llama_index.core.node_parser import SimpleNodeParser\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "def _build_index(chunk_size, docs):\n",
        "    index_out_path = f\"./storage_{chunk_size}\"\n",
        "    if not os.path.exists(index_out_path):\n",
        "        Path(index_out_path).mkdir(parents=True, exist_ok=True)\n",
        "        # parse docs\n",
        "        node_parser = SimpleNodeParser.from_defaults(chunk_size=chunk_size)\n",
        "        base_nodes = node_parser.get_nodes_from_documents(docs)\n",
        "\n",
        "        # build index\n",
        "        index = VectorStoreIndex(base_nodes)\n",
        "        # save index to disk\n",
        "        index.storage_context.persist(index_out_path)\n",
        "    else:\n",
        "        # rebuild storage context\n",
        "        storage_context = StorageContext.from_defaults(\n",
        "            persist_dir=index_out_path\n",
        "        )\n",
        "        # load index\n",
        "        index = load_index_from_storage(\n",
        "            storage_context,\n",
        "        )\n",
        "    return index"
      ],
      "metadata": {
        "id": "lUN1VlCj8dRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import ServiceContext\n",
        "from llama_index.experimental.param_tuner.base import ParamTuner, RunResult\n",
        "from llama_index.core.evaluation import SemanticSimilarityEvaluator, BatchEvalRunner\n",
        "from llama_index.core.evaluation.eval_utils import get_responses\n",
        "import numpy as np\n",
        "\n",
        "### Recipe\n",
        "### Perform hyperparameter tuning as in traditional ML via grid-search\n",
        "### 1. Define an objective function that ranks different parameter combos\n",
        "### 2. Build ParamTuner object\n",
        "### 3. Execute hyperparameter tuning with ParamTuner.tune()\n",
        "\n",
        "# 1. Define objective function\n",
        "def objective_function(params_dict):\n",
        "    chunk_size = params_dict[\"chunk_size\"]\n",
        "    docs = params_dict[\"docs\"]\n",
        "    top_k = params_dict[\"top_k\"]\n",
        "    eval_qs = params_dict[\"eval_qs\"]\n",
        "    ref_response_strs = params_dict[\"ref_response_strs\"]\n",
        "\n",
        "    # build RAG pipeline\n",
        "    index = _build_index(chunk_size, docs)  # helper function not shown here\n",
        "    query_engine = index.as_query_engine(similarity_top_k=top_k)\n",
        "\n",
        "    # perform inference with RAG pipeline on a provided questions `eval_qs`\n",
        "    pred_response_objs = get_responses(\n",
        "        eval_qs, query_engine, show_progress=True\n",
        "    )\n",
        "\n",
        "    # perform evaluations of predictions by comparing them to reference\n",
        "    # responses `ref_response_strs`\n",
        "    evaluator = SemanticSimilarityEvaluator(embed_model=OpenAIEmbedding())\n",
        "    eval_batch_runner = BatchEvalRunner(\n",
        "        {\"semantic_similarity\": evaluator}, workers=2, show_progress=True\n",
        "    )\n",
        "    eval_results = eval_batch_runner.evaluate_responses(\n",
        "        eval_qs, responses=pred_response_objs, reference=ref_response_strs\n",
        "    )\n",
        "\n",
        "    # get semantic similarity metric\n",
        "    mean_score = np.array(\n",
        "        [r.score for r in eval_results[\"semantic_similarity\"]]\n",
        "    ).mean()\n",
        "\n",
        "    return RunResult(score=mean_score, params=params_dict)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fxDaI8B4SV69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_qs = [\n",
        "    \"What is the primary goal of the EU's proposed AI Act of April 2021?\",\n",
        "    \"Which AI systems are considered to pose 'unacceptable' risks under the EU's proposed AI Act?\",\n",
        "    \"What are the obligations for 'high-risk' AI systems according to the EU's proposed AI Act?\",\n",
        "    \"What does the EU's proposed AI Act require from AI systems that present only 'limited risk'?\",\n",
        "    \"When did the EU Member States agree on their general position regarding the AI Act?\",\n",
        "    \"What are the significant amendments proposed by the EU Parliament to the AI Act?\",\n",
        "    \"How does the proposed AI Act define AI systems?\",\n",
        "    \"What are the concerns raised about the AI Act's definition of AI systems?\",\n",
        "    \"What is the risk-based approach proposed by the EU's AI Act?\",\n",
        "    \"What are the proposed sanctions for non-compliance with the AI Act?\"\n",
        "]\n",
        "\n",
        "ref_response_strs = [\n",
        "    \"The primary goal is to ensure the proper functioning of the single market by creating conditions for the development and use of trustworthy AI systems in the Union.\",\n",
        "    \"AI systems that deploy subliminal techniques or exploit vulnerable groups, among others, are considered to pose unacceptable risks.\",\n",
        "    \"High-risk AI systems must undergo an ex-ante conformity assessment, be registered in an EU database, and comply with specific requirements such as risk management and data governance.\",\n",
        "    \"AI systems presenting limited risk are subject to transparency obligations, such as clear user information.\",\n",
        "    \"The EU Member States agreed on their general position in December 2021.\",\n",
        "    \"Significant amendments include revising the definition of AI systems, broadening the list of prohibited AI systems, and imposing obligations on general-purpose and generative AI models.\",\n",
        "    \"AI systems are defined as software developed with specific techniques and approaches that can generate outputs influencing the environments they interact with.\",\n",
        "    \"Concerns include the broad definition that could cover simple algorithms and the lack of clarity that may lead to legal uncertainty.\",\n",
        "    \"The risk-based approach classifies AI systems based on their level of risk, from unacceptable to high, limited, and minimal, tailoring legal interventions accordingly.\",\n",
        "    \"Sanctions include administrative fines up to €30 million or 6% of the total worldwide annual turnover, depending on the infringement's severity.\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "PT4A0Qvw79vd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs= SimpleDirectoryReader(input_files=[\"./transcripts/AIAct.pdf\"]).load_data()\n",
        "\n",
        "\n",
        "# 2. Build ParamTuner object\n",
        "param_dict = {\"chunk_size\": [256, 512, 1024]} # params/values to search over\n",
        "fixed_param_dict = { # fixed hyperparams\n",
        "  \"top_k\": 2,\n",
        "    \"docs\": docs,\n",
        "    \"eval_qs\": eval_qs[:10],\n",
        "    \"ref_response_strs\": ref_response_strs[:10],\n",
        "}\n",
        "param_tuner = ParamTuner(\n",
        "    param_fn=objective_function,\n",
        "    param_dict=param_dict,\n",
        "    fixed_param_dict=fixed_param_dict,\n",
        "    show_progress=True,\n",
        ")\n",
        "\n",
        "# 3. Execute hyperparameter search\n",
        "results = param_tuner.tune()\n",
        "best_result = results.best_run_result\n",
        "best_chunk_size = results.best_run_result.params[\"chunk_size\"]"
      ],
      "metadata": {
        "id": "CDThbPmYzY7A",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "6232dd80beb4419f89efad73681d2cc3",
            "8879a067f10c47efa9afe51aba546f4c",
            "768ab6e06c2347a6b8b3ab06aae68899",
            "cefeea4488194ef4bd9409b229f6b25b",
            "806e2687478643529a3e0450ed2f6b7f",
            "56e4f6c87dc74ec9808bc29a5b6fcc41",
            "bc18cbe4f74d481fa4b24637559ab3d9",
            "9b0a256348514c21bfec55bb11853e73",
            "9d3837b9bbe64bc5bdb4152962a7c367",
            "a1c2ef74106043a6a2ec76be6e125fde",
            "233e301ab5af4f40ae27534e489f9987"
          ]
        },
        "outputId": "c8641c09-5980-4d9b-8eec-1160358b96bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Param combinations.:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6232dd80beb4419f89efad73681d2cc3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 10%|█         | 1/10 [00:02<00:22,  2.48s/it]\u001b[A\n",
            " 20%|██        | 2/10 [00:02<00:09,  1.25s/it]\u001b[A\n",
            " 30%|███       | 3/10 [00:03<00:05,  1.23it/s]\u001b[A\n",
            " 60%|██████    | 6/10 [00:03<00:01,  3.30it/s]\u001b[A\n",
            " 80%|████████  | 8/10 [00:03<00:00,  4.80it/s]\u001b[A\n",
            "100%|██████████| 10/10 [00:03<00:00,  2.83it/s]\n",
            "\n",
            "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 10%|█         | 1/10 [00:00<00:05,  1.58it/s]\u001b[A\n",
            " 30%|███       | 3/10 [00:00<00:02,  3.48it/s]\u001b[A\n",
            " 50%|█████     | 5/10 [00:01<00:01,  4.05it/s]\u001b[A\n",
            " 60%|██████    | 6/10 [00:01<00:01,  3.41it/s]\u001b[A\n",
            " 70%|███████   | 7/10 [00:02<00:00,  3.14it/s]\u001b[A\n",
            " 80%|████████  | 8/10 [00:02<00:00,  3.14it/s]\u001b[A\n",
            " 90%|█████████ | 9/10 [00:02<00:00,  3.67it/s]\u001b[A\n",
            "100%|██████████| 10/10 [00:02<00:00,  3.54it/s]\n",
            "\n",
            "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 10%|█         | 1/10 [00:01<00:15,  1.74s/it]\u001b[A\n",
            " 20%|██        | 2/10 [00:01<00:06,  1.19it/s]\u001b[A\n",
            " 30%|███       | 3/10 [00:02<00:04,  1.54it/s]\u001b[A\n",
            " 50%|█████     | 5/10 [00:02<00:01,  3.16it/s]\u001b[A\n",
            " 60%|██████    | 6/10 [00:02<00:01,  3.53it/s]\u001b[A\n",
            " 70%|███████   | 7/10 [00:02<00:00,  3.53it/s]\u001b[A\n",
            " 80%|████████  | 8/10 [00:03<00:00,  4.25it/s]\u001b[A\n",
            " 90%|█████████ | 9/10 [00:03<00:00,  5.03it/s]\u001b[A\n",
            "100%|██████████| 10/10 [00:03<00:00,  2.79it/s]\n",
            "\n",
            "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 10%|█         | 1/10 [00:00<00:06,  1.41it/s]\u001b[A\n",
            " 30%|███       | 3/10 [00:01<00:02,  3.10it/s]\u001b[A\n",
            " 50%|█████     | 5/10 [00:01<00:01,  3.96it/s]\u001b[A\n",
            " 70%|███████   | 7/10 [00:01<00:00,  4.41it/s]\u001b[A\n",
            "100%|██████████| 10/10 [00:02<00:00,  4.44it/s]\n",
            "\n",
            "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 10%|█         | 1/10 [00:01<00:11,  1.24s/it]\u001b[A\n",
            " 30%|███       | 3/10 [00:01<00:02,  2.66it/s]\u001b[A\n",
            " 50%|█████     | 5/10 [00:02<00:02,  2.29it/s]\u001b[A\n",
            " 60%|██████    | 6/10 [00:02<00:01,  2.66it/s]\u001b[A\n",
            " 80%|████████  | 8/10 [00:02<00:00,  3.86it/s]\u001b[A\n",
            "100%|██████████| 10/10 [00:03<00:00,  3.01it/s]\n",
            "\n",
            "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 10%|█         | 1/10 [00:00<00:03,  2.48it/s]\u001b[A\n",
            " 20%|██        | 2/10 [00:00<00:02,  2.81it/s]\u001b[A\n",
            " 30%|███       | 3/10 [00:01<00:02,  3.14it/s]\u001b[A\n",
            " 50%|█████     | 5/10 [00:01<00:01,  4.48it/s]\u001b[A\n",
            " 60%|██████    | 6/10 [00:01<00:01,  3.81it/s]\u001b[A\n",
            " 70%|███████   | 7/10 [00:01<00:00,  4.25it/s]\u001b[A\n",
            " 80%|████████  | 8/10 [00:02<00:00,  4.14it/s]\u001b[A\n",
            " 90%|█████████ | 9/10 [00:02<00:00,  4.00it/s]\u001b[A\n",
            "100%|██████████| 10/10 [00:02<00:00,  3.96it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"best chunk size is: \", best_chunk_size)"
      ],
      "metadata": {
        "id": "LodNRGuCzY4M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07860543-12f2-4a73-f4a6-429b07cffe8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best chunk size is:  256\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Response Synthesizer\n",
        "A Response Synthesizer is what generates a response from an LLM, using a user query and a given set of text chunks. The output of a response synthesizer is a Response object.\n",
        "\n",
        "The method for doing this can take many forms, from as simple as iterating over text chunks, to as complex as building a tree. The main idea here is to simplify the process of generating a response using an LLM across your data.\n",
        "\n",
        "# Tree summarize\n",
        "Query the LLM using the `summary_template` prompt as many times as needed so that all concatenated chunks have been queried, resulting in as many answers that are themselves recursively used as chunks in a tree_summarize LLM call and so on, until there's only one chunk left, and thus only one final answer.\n",
        "\n",
        "![Tree Summarize](https://static.bluelabellabs.com/wp-content/uploads/2024/02/Tree-Summarize.png)\n",
        "*Image Source: [Source](https://www.bluelabellabs.com/blog/llamaindex-response-modes-explained/)*\n"
      ],
      "metadata": {
        "id": "4YA-RSFQZWxR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import get_response_synthesizer\n",
        "from llama_index.core.retrievers import VectorIndexRetriever\n",
        "from llama_index.core.query_engine import RetrieverQueryEngine\n",
        "from llama_index.core.response.pprint_utils import pprint_response\n",
        "\n",
        "# configure retriever\n",
        "retriever = VectorIndexRetriever(\n",
        "    index=index,\n",
        "    similarity_top_k=5)\n",
        "\n",
        "# configure response synthesizer\n",
        "response_synthesizer = get_response_synthesizer(\n",
        "    response_mode=\"tree_summarize\")\n",
        "\n",
        "# assemble query engine\n",
        "query_engine = RetrieverQueryEngine(\n",
        "    retriever=retriever,\n",
        "    response_synthesizer=response_synthesizer,)\n",
        "\n",
        "response = query_engine.query(question)\n",
        "\n",
        "pprint_response(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGSgR8ExZOie",
        "outputId": "c3ece887-dbd9-4dc4-c1b2-16777bc0e6ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Response: The document mentions that there are concerns about\n",
            "the definitions of 'emotion recognition' being technically flawed and\n",
            "recommends adjustments. Additionally, there are calls for a wider ban\n",
            "on the use of AI for emotion recognition, categorizing people based on\n",
            "physiological, behavioral, or biometric data, and for stronger impact\n",
            "assessment and transparency requirements in this regard.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Semantic Reranker\n",
        "\n",
        "The Semantic Reranker is an advanced component that enhances the relevance of the retrieved document chunks before they are used for response generation. By applying semantic analysis, the reranker evaluates and prioritizes chunks based on their contextual alignment with the user's query, ensuring that the most pertinent information is considered. This process not only improves the accuracy and relevance of the generated responses but also addresses common issues such as information redundancy and irrelevance. This section will delve into the implementation and integration of a Semantic Reranker within the RAG framework.\n",
        "\n",
        "![Placeholder for Image](https://framerusercontent.com/images/4atMCrE67i6XDQdSySevR8sVhM.png)\n",
        "*Image Source: [Source](https://dify.ai/blog/hybrid-search-rerank-rag-improvement)*\n"
      ],
      "metadata": {
        "id": "HGdE4C-xJsNO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -q llama-index-postprocessor-cohere-rerank"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uOJZivOrSsCO",
        "outputId": "00579388-c6f5-4902-e501-0c4666635400"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.3/145.3 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.postprocessor.cohere_rerank import CohereRerank\n",
        "import os\n",
        "\n",
        "api_key = os.getenv(\"COHERE_API_KEY\")\n",
        "cohere_rerank = CohereRerank(api_key=api_key, top_n=2)\n",
        "\n",
        "# assemble query engine\n",
        "query_engine = RetrieverQueryEngine(\n",
        "    retriever=retriever,\n",
        "    response_synthesizer=response_synthesizer,\n",
        "    node_postprocessors=[cohere_rerank])\n",
        "\n",
        "response = query_engine.query(question)\n",
        "\n",
        "pprint_response(response)"
      ],
      "metadata": {
        "id": "nk5pE2QlzY0-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb732131-0cdb-4a2a-9fb3-d22003982f02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Response: The document mentions that some stakeholders argue\n",
            "that the definitions of 'emotion recognition' are technically flawed\n",
            "and recommend adjustments. Additionally, it states that AI systems\n",
            "presenting 'limited risk', such as emotion recognition systems, would\n",
            "be subject to a limited set of transparency obligations.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Integration with langchain (Optional)\n",
        "\n",
        "To further expand our generative AI application's capabilities, we can integrate LangChain, a framework that provides additional flexibility and power through its tools, agents and chains. LangChain allows for the creation of complex workflows, enabling our app to handle more intricate queries and perform a broader range of tasks. This integration showcases the extensibility of RAG systems and their potential for customization to meet specific application needs.\n",
        "\n",
        "![LangChain as a central hub](https://www.kdnuggets.com/wp-content/uploads/c_langchain_101_build_gptpowered_applications_1.jpg)\n",
        "*Image Source: [Source](https://www.kdnuggets.com/2023/04/langchain-101-build-gptpowered-applications.html)*\n",
        "\n"
      ],
      "metadata": {
        "id": "m35rHER9qCpR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gl-APXINrVMb",
        "outputId": "04088f22-2785-44d9-85c0-4628b77f5705"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m812.8/812.8 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.6/274.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.9/86.9 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.8/144.8 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.langchain_helpers.agents import IndexToolConfig, LlamaIndexTool\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
        "\n",
        "tool_config = IndexToolConfig(\n",
        "    query_engine=query_engine,\n",
        "    name=f\"Vector Index\",\n",
        "    description=f\"useful for when you want to answer queries about the document\",\n",
        "    tool_kwargs={\"return_direct\": True},\n",
        "    memory = memory)\n",
        "\n",
        "# create the tool\n",
        "tool = LlamaIndexTool.from_tool_config(tool_config)\n",
        "\n",
        "response=tool.run(question)\n",
        "\n",
        "custom_print(response)"
      ],
      "metadata": {
        "id": "tIfCryolJg5J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55999fd0-295e-42d1-eeef-1b39ada9a0ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The document mentions that some stakeholders argue the definitions of 'emotion \n",
            "recognition' are technically flawed and recommend adjustments. Additionally, it \n",
            "states that AI systems presenting 'limited risk', such as emotion recognition \n",
            "systems, would be subject to a limited set of transparency obligations. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pQXYml8VXdz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Making ready for the production\n",
        "\n",
        "In this section, we will get the code ready for the production. This guides you through the process of packaging our RAG application, ensuring it's ready for a production environment. We'll cover essential steps such as dependency management, application wrapping with Streamlit for user interaction, and setting up a secure tunnel for public access. This prepares our RAG system to be deployed and utilized in practical scenarios, bringing the power of advanced AI to end-users.\n"
      ],
      "metadata": {
        "id": "IRTw6JttvdgP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -r requirements.txt"
      ],
      "metadata": {
        "id": "a-rf046zwitM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-fTDzKKEwj47"
      },
      "outputs": [],
      "source": [
        "%%writefile app.py\n",
        "\n",
        "\n",
        "import streamlit as st\n",
        "from llama_index.core import(\n",
        "    SimpleDirectoryReader,\n",
        "    VectorStoreIndex,\n",
        "    ServiceContext,\n",
        "    Document,\n",
        "    get_response_synthesizer,\n",
        "    StorageContext\n",
        ")\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "\n",
        "from llama_index.core.retrievers import VectorIndexRetriever\n",
        "from llama_index.core.query_engine import RetrieverQueryEngine\n",
        "from llama_index.core.langchain_helpers.agents import IndexToolConfig, LlamaIndexTool\n",
        "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
        "from llama_index.postprocessor.cohere_rerank import CohereRerank\n",
        "import qdrant_client\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "import time\n",
        "import nest_asyncio\n",
        "from dotenv import load_dotenv\n",
        "import tempfile\n",
        "import os\n",
        "\n",
        "nest_asyncio.apply()\n",
        "#import openai\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "llm=OpenAI(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    temperature=0.2\n",
        "    )\n",
        "embed_model = OpenAIEmbedding()\n",
        "\n",
        "client = qdrant_client.QdrantClient(location=\":memory:\")\n",
        "vector_store = QdrantVectorStore(client=client, collection_name=\"test_store\")\n",
        "\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "cohere_api_key = os.getenv(\"COHERE_API_KEY\")\n",
        "cohere_rerank = CohereRerank(api_key=cohere_api_key, top_n=2)\n",
        "\n",
        "# Set the header of the Streamlit application\n",
        "st.header(\"Workshop Transcript Chatbot\")\n",
        "\n",
        "# Initialize session state to store the chat history\n",
        "if \"messages\" not in st.session_state.keys(): # Initialize the chat message history\n",
        "    st.session_state.messages = [\n",
        "        {\"role\": \"assistant\", \"content\": \"Ask me a question about DPS Workshops!\"}\n",
        "    ]\n",
        "\n",
        "@st.cache_resource(show_spinner=False)\n",
        "def load_data(uploaded_files):\n",
        "    \"\"\"\n",
        "    Load and index workshop transcripts uploaded by the user.\n",
        "\n",
        "    Args:\n",
        "        uploaded_files: A list of uploaded file objects.\n",
        "\n",
        "    Returns:\n",
        "        VectorStoreIndex: An indexed representation of the workshop transcripts.\n",
        "    \"\"\"\n",
        "    with st.spinner(text=\"Indexing uploaded workshop docs – hang tight! This might take some time.\"):\n",
        "        with tempfile.TemporaryDirectory() as temp_dir:\n",
        "            for uploaded_file in uploaded_files:\n",
        "                if uploaded_file is not None:\n",
        "                    file_path = os.path.join(temp_dir, uploaded_file.name)\n",
        "                    with open(file_path, \"wb\") as f:\n",
        "                        f.write(uploaded_file.getbuffer())\n",
        "\n",
        "            reader = SimpleDirectoryReader(input_dir=temp_dir, recursive=True)\n",
        "            docs = reader.load_data()\n",
        "\n",
        "            if docs:\n",
        "                service_context_for_indexing = ServiceContext.from_defaults(embed_model = embed_model)\n",
        "                # Execute pipeline and time the process\n",
        "                index =  VectorStoreIndex.from_documents(docs,storage_context=storage_context)\n",
        "\n",
        "                return index\n",
        "            else:\n",
        "                return None\n",
        "\n",
        "# Streamlit file uploader\n",
        "uploaded_files = st.file_uploader(\"Upload workshop documents\", accept_multiple_files=True)\n",
        "\n",
        "if uploaded_files:\n",
        "    index = load_data(uploaded_files)\n",
        "\n",
        "    if index:\n",
        "        # Set up the ServiceContext with the LLM for the querying stage\n",
        "        service_context_for_querying = ServiceContext.from_defaults(\n",
        "            llm=llm,\n",
        "            embed_model=embed_model\n",
        "            )\n",
        "\n",
        "        # configure retriever\n",
        "        retriever = VectorIndexRetriever(\n",
        "            index=index,\n",
        "            similarity_top_k=5,\n",
        "        )\n",
        "\n",
        "        # configure response synthesizer\n",
        "        response_synthesizer = get_response_synthesizer(\n",
        "            response_mode=\"tree_summarize\",\n",
        "        )\n",
        "\n",
        "        # assemble query engine\n",
        "        query_engine = RetrieverQueryEngine(\n",
        "            retriever=retriever,\n",
        "            response_synthesizer=response_synthesizer,\n",
        "            node_postprocessors=[cohere_rerank])\n",
        "\n",
        "\n",
        "        memory = ConversationBufferMemory(\n",
        "            memory_key='chat_history', return_messages=True\n",
        "            )\n",
        "\n",
        "\n",
        "        tool_config = IndexToolConfig(\n",
        "            query_engine=query_engine,\n",
        "            name=f\"Vector Index\",\n",
        "            description=f\"useful for when you want to answer queries about the document\",\n",
        "            tool_kwargs={\"return_direct\": True},\n",
        "            memory = memory\n",
        "            )\n",
        "\n",
        "        # create the tool\n",
        "        tool = LlamaIndexTool.from_tool_config(tool_config)\n",
        "\n",
        "        # Chat interface for user input and displaying chat history\n",
        "        if prompt := st.chat_input(\"Your question\"):\n",
        "            st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "        for message in st.session_state.messages:\n",
        "            with st.chat_message(message[\"role\"]):\n",
        "                st.write(message[\"content\"])\n",
        "\n",
        "        # Generate and display the response from the chat engine\n",
        "        if st.session_state.messages[-1][\"role\"] != \"assistant\":\n",
        "            with st.chat_message(\"assistant\"):\n",
        "                with st.spinner(\"Thinking...\"):\n",
        "                    # Retrieve the response from the chat engine based on the user's prompt\n",
        "                    response=tool.run(prompt)\n",
        "                    #st.write(response.response)\n",
        "                    st.write(response)\n",
        "                    #message = {\"role\": \"assistant\", \"content\": response.response}\n",
        "                    message = {\"role\": \"assistant\", \"content\": response}\n",
        "                    st.session_state.messages.append(message) # Add response to message history\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DjOR0TYfvrmJ"
      },
      "outputs": [],
      "source": [
        "!npm install localtunnel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sto002Tl1O_N"
      },
      "outputs": [],
      "source": [
        "!streamlit run app.py &>/content/logs.txt &"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qOu94jV635F9"
      },
      "outputs": [],
      "source": [
        "!curl https://loca.lt/mytunnelpassword\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ju6MdPrw1SZc"
      },
      "outputs": [],
      "source": [
        "!npx localtunnel --port 8501"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6232dd80beb4419f89efad73681d2cc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8879a067f10c47efa9afe51aba546f4c",
              "IPY_MODEL_768ab6e06c2347a6b8b3ab06aae68899",
              "IPY_MODEL_cefeea4488194ef4bd9409b229f6b25b"
            ],
            "layout": "IPY_MODEL_806e2687478643529a3e0450ed2f6b7f"
          }
        },
        "8879a067f10c47efa9afe51aba546f4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_56e4f6c87dc74ec9808bc29a5b6fcc41",
            "placeholder": "​",
            "style": "IPY_MODEL_bc18cbe4f74d481fa4b24637559ab3d9",
            "value": "Param combinations.: 100%"
          }
        },
        "768ab6e06c2347a6b8b3ab06aae68899": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9b0a256348514c21bfec55bb11853e73",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9d3837b9bbe64bc5bdb4152962a7c367",
            "value": 3
          }
        },
        "cefeea4488194ef4bd9409b229f6b25b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a1c2ef74106043a6a2ec76be6e125fde",
            "placeholder": "​",
            "style": "IPY_MODEL_233e301ab5af4f40ae27534e489f9987",
            "value": " 3/3 [00:24&lt;00:00,  7.49s/it]"
          }
        },
        "806e2687478643529a3e0450ed2f6b7f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56e4f6c87dc74ec9808bc29a5b6fcc41": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc18cbe4f74d481fa4b24637559ab3d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9b0a256348514c21bfec55bb11853e73": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d3837b9bbe64bc5bdb4152962a7c367": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a1c2ef74106043a6a2ec76be6e125fde": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "233e301ab5af4f40ae27534e489f9987": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}